# Importations essentielles
import pandas as pd
from sklearn.ensemble import IsolationForest
import streamlit as st

# Import des modules
from SVM import train_svm_model, create_confusion_matrix_plot, create_roc_curve_plot, create_precision_recall_plot
from ANN import train_ann_model, create_ann_plots
from XGBoost import (train_xgboost_model, create_xgboost_confusion_matrix_plot, 
                     create_xgboost_roc_curve_plot, create_xgboost_precision_recall_plot, 
                     create_feature_importance_plot)

# Configuration Streamlit
st.set_page_config(page_title="D√©tection de Fraude", layout="wide")
st.title("üîç D√©tection de Fraude Bancaire")
st.write("""
### Analyse des transactions avec Machine Learning
Cette application d√©tecte les op√©rations suspectes en utilisant diff√©rents algorithmes.
""")

# Chargement des donn√©es
@st.cache_data
def load_data():
    return pd.read_csv("creditcard_Nettoy√©.csv")

# Cache pour les mod√®les
@st.cache_data
def get_svm_results(data):
    return train_svm_model(data)

@st.cache_data
def get_ann_results(data):
    return train_ann_model(data)

@st.cache_data
def get_xgboost_results(data):
    return train_xgboost_model(data)

# Interface principale
data = load_data()

# Sidebar pour les informations sur les donn√©es
with st.sidebar:
    st.header("Informations sur les donn√©es")
    st.write(f"**Nombre total de transactions:** {len(data):,}")
    st.write(f"**Transactions normales:** {(data['Class'] == 0).sum():,}")
    st.write(f"**Transactions frauduleuses:** {(data['Class'] == 1).sum():,}")
    st.write(f"**Taux de fraude:** {(data['Class'] == 1).mean()*100:.2f}%")
    
    st.divider()
    
    st.header("Algorithmes disponibles")
    st.write("**SVM** - Support Vector Machine")
    st.write("**ANN** - Artificial Neural Network")
    st.write("**XGBoost** - Extreme Gradient Boosting")

# Section principale
col1, col2 = st.columns([2, 1])

with col1:
    # Affichage des donn√©es
    if st.checkbox("Afficher les donn√©es brutes"):
        st.dataframe(data.head(100), use_container_width=True)

with col2:
    st.write("### Lancer l'analyse")
    
    # Boutons pour les diff√©rents algorithmes
    col_svm, col_ann, col_xgb = st.columns(3)
    
    with col_svm:
        if st.button("SVM", type="primary", use_container_width=True):
            with st.spinner('Entra√Ænement SVM...'):
                try:
                    results = get_svm_results(data)
                    st.success("‚úÖ SVM termin√©!")
                    st.session_state.svm_results = results
                    # Nettoyer les autres r√©sultats
                    for key in ['ann_results', 'xgboost_results', 'comparison_mode']:
                        if key in st.session_state:
                            del st.session_state[key]
                except Exception as e:
                    st.error(f"Erreur SVM : {str(e)}")
    
    with col_ann:
        if st.button("ANN", type="secondary", use_container_width=True):
            with st.spinner('Entra√Ænement ANN...'):
                try:
                    results = get_ann_results(data)
                    st.success("ANN termin√©!")
                    st.session_state.ann_results = results
                    # Nettoyer les autres r√©sultats
                    for key in ['svm_results', 'xgboost_results', 'comparison_mode']:
                        if key in st.session_state:
                            del st.session_state[key]
                except Exception as e:
                    st.error(f"Erreur ANN : {str(e)}")
    
    with col_xgb:
        if st.button("XGBoost", type="secondary", use_container_width=True):
            with st.spinner('Entra√Ænement XGBoost...'):
                try:
                    results = get_xgboost_results(data)
                    st.success(" XGBoost termin√©!")
                    st.session_state.xgboost_results = results
                    # Nettoyer les autres r√©sultats
                    for key in ['svm_results', 'ann_results', 'comparison_mode']:
                        if key in st.session_state:
                            del st.session_state[key]
                except Exception as e:
                    st.error(f"Erreur XGBoost : {str(e)}")
    
    # Bouton pour comparer tous les mod√®les
    st.divider()
    if st.button("Comparer tous les mod√®les", use_container_width=True):
        with st.spinner('Entra√Ænement de tous les mod√®les...'):
            try:
                svm_results = get_svm_results(data)
                ann_results = get_ann_results(data)
                xgboost_results = get_xgboost_results(data)
                
                # Stocker tous les r√©sultats
                st.session_state.svm_results = svm_results
                st.session_state.ann_results = ann_results
                st.session_state.xgboost_results = xgboost_results
                st.session_state.comparison_mode = True
                
                st.success("Comparaison termin√©e!")
                
            except Exception as e:
                st.error(f"Erreur lors de la comparaison : {str(e)}")

# Affichage des r√©sultats SVM
if 'svm_results' in st.session_state and 'comparison_mode' not in st.session_state:
    results = st.session_state.svm_results
    
    st.write("## R√©sultats de l'analyse SVM")
    
    # M√©triques principales
    st.write("###  Performances du mod√®le")
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Exactitude", f"{results['metrics']['accuracy']:.3f}")
    with col2:
        st.metric("Pr√©cision", f"{results['metrics']['precision']:.3f}")
    with col3:
        st.metric("Rappel", f"{results['metrics']['recall']:.3f}")
    with col4:
        st.metric("F1-Score", f"{results['metrics']['f1']:.3f}")
    
    # Graphiques SVM
    st.write("###  Visualisations")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.write("**Matrice de Confusion**")
        fig1 = create_confusion_matrix_plot(results['confusion_matrix'])
        st.pyplot(fig1)
    
    with col2:
        st.write("**Courbe ROC**")
        fig2 = create_roc_curve_plot(results['roc_data'], results['metrics']['roc_auc'])
        st.pyplot(fig2)
    
    with col3:
        st.write("**Courbe Pr√©cision-Rappel**")
        fig3 = create_precision_recall_plot(results['pr_data'], results['metrics']['pr_auc'])
        st.pyplot(fig3)

# Affichage des r√©sultats ANN
if 'ann_results' in st.session_state and 'comparison_mode' not in st.session_state:
    results = st.session_state.ann_results
    
    st.write("##  R√©sultats de l'analyse ANN (R√©seau de Neurones)")
    
    # M√©triques principales
    st.write("###  Performances du mod√®le")
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Exactitude", f"{results['metrics']['accuracy']:.3f}")
    with col2:
        st.metric("Pr√©cision", f"{results['metrics']['precision']:.3f}")
    with col3:
        st.metric("Rappel", f"{results['metrics']['recall']:.3f}")
    with col4:
        st.metric("F1-Score", f"{results['metrics']['f1']:.3f}")

    # Informations sur le mod√®le et les donn√©es
    col1, col2 = st.columns(2)
    with col1:
        st.write("###  Architecture du mod√®le")
        st.write(f"**Couches cach√©es:** {results['model_info']['hidden_layers']}")
        st.write(f"**Fonction d'activation:** {results['model_info']['activation']}")
        st.write(f"**√âpoques d'entra√Ænement:** {results['model_info']['epochs_trained']}")
        st.write(f"**Taille de batch:** {results['model_info']['batch_size']}")
    
    with col2:
        st.write("###  Informations d'entra√Ænement")
        st.write(f"**√âchantillons totaux:** {results['data_info']['total_samples']:,}")
        st.write(f"**√âchantillons d'entra√Ænement:** {results['data_info']['train_samples']:,}")
        st.write(f"**√âchantillons de validation:** {results['data_info']['val_samples']:,}")
        st.write(f"**√âchantillons de test:** {results['data_info']['test_samples']:,}")

    # M√©triques avanc√©es
    st.write("### M√©triques avanc√©es")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("AUC-ROC", f"{results['metrics']['roc_auc']:.4f}")
    with col2:
        st.metric("AUC-PR", f"{results['metrics']['pr_auc']:.4f}")
    with col3:
        st.write(f"**Taux de fraude:** {results['data_info']['fraud_rate']*100:.2f}%")
    
    # Graphiques ANN
    st.write("###  Visualisations")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**Courbes d'apprentissage**")
        fig1 = create_ann_plots(results, 'learning_curves')
        st.pyplot(fig1)
    
    with col2:
        st.write("**Matrice de Confusion**")
        fig2 = create_ann_plots(results, 'confusion_matrix')
        st.pyplot(fig2)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**Courbe ROC**")
        fig3 = create_ann_plots(results, 'roc_curve')
        st.pyplot(fig3)
    
    with col2:
        st.write("**Courbe Pr√©cision-Rappel**")
        fig4 = create_ann_plots(results, 'precision_recall')
        st.pyplot(fig4)

# Affichage des r√©sultats XGBoost
if 'xgboost_results' in st.session_state and 'comparison_mode' not in st.session_state:
    results = st.session_state.xgboost_results
    
    st.write("##  R√©sultats de l'analyse XGBoost")
    
    # M√©triques principales
    st.write("###  Performances du mod√®le")
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Exactitude", f"{results['metrics']['accuracy']:.3f}")
    with col2:
        st.metric("Pr√©cision", f"{results['metrics']['precision']:.3f}")
    with col3:
        st.metric("Rappel", f"{results['metrics']['recall']:.3f}")
    with col4:
        st.metric("F1-Score", f"{results['metrics']['f1']:.3f}")
    
    # Informations sur les donn√©es utilis√©es
    col1, col2 = st.columns(2)
    with col1:
        st.write("###  Informations d'entra√Ænement")
        st.write(f"**√âchantillons totaux:** {results['data_info']['total_samples']:,}")
        st.write(f"**√âchantillons d'entra√Ænement:** {results['data_info']['train_samples']:,}")
        st.write(f"**√âchantillons de validation:** {results['data_info']['val_samples']:,}")
        st.write(f"**√âchantillons de test:** {results['data_info']['test_samples']:,}")
    
    with col2:
        st.write("###  M√©triques avanc√©es")
        st.metric("AUC-ROC", f"{results['metrics']['roc_auc']:.4f}")
        st.metric("AUC-PR", f"{results['metrics']['pr_auc']:.4f}")
        st.write(f"**Taux de fraude:** {results['data_info']['fraud_rate']*100:.2f}%")
    
    # Graphiques XGBoost
    st.write("### Visualisations")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**Matrice de Confusion**")
        fig1 = create_xgboost_confusion_matrix_plot(results['confusion_matrix'])
        st.pyplot(fig1)
    
    with col2:
        st.write("**Courbe ROC**")
        fig2 = create_xgboost_roc_curve_plot(results['roc_data'], results['metrics']['roc_auc'])
        st.pyplot(fig2)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**Courbe Pr√©cision-Rappel**")
        fig3 = create_xgboost_precision_recall_plot(results['pr_data'], results['metrics']['pr_auc'])
        st.pyplot(fig3)
    
    with col2:
        st.write("**Importance des Features**")
        fig4 = create_feature_importance_plot(results['feature_importance'])
        st.pyplot(fig4)
    
    # Rapport de classification d√©taill√©
    st.write("###  Rapport de classification d√©taill√©")
    st.text(results['classification_report'])

# Mode comparaison
if 'comparison_mode' in st.session_state:
    st.write("##  Comparaison des mod√®les")
    
    svm_results = st.session_state.svm_results
    ann_results = st.session_state.ann_results
    xgboost_results = st.session_state.xgboost_results
    
    # Tableau de comparaison des m√©triques
    st.write("###  Comparaison des performances")
    
    comparison_df = pd.DataFrame({
        'M√©trique': ['Exactitude', 'Pr√©cision', 'Rappel', 'F1-Score', 'AUC-ROC', 'AUC-PR'],
        'SVM': [
            svm_results['metrics']['accuracy'],
            svm_results['metrics']['precision'],
            svm_results['metrics']['recall'],
            svm_results['metrics']['f1'],
            svm_results['metrics']['roc_auc'],
            svm_results['metrics']['pr_auc']
        ],
        'ANN': [
            ann_results['metrics']['accuracy'],
            ann_results['metrics']['precision'],
            ann_results['metrics']['recall'],
            ann_results['metrics']['f1'],
            ann_results['metrics']['roc_auc'],
            ann_results['metrics']['pr_auc']
        ],
        'XGBoost': [
            xgboost_results['metrics']['accuracy'],
            xgboost_results['metrics']['precision'],
            xgboost_results['metrics']['recall'],
            xgboost_results['metrics']['f1'],
            xgboost_results['metrics']['roc_auc'],
            xgboost_results['metrics']['pr_auc']
        ]
    })
    
    # Ajouter une colonne "Meilleur"
    comparison_df['Meilleur'] = comparison_df.apply(
        lambda row: 'SVM' if row['SVM'] >= max(row['ANN'], row['XGBoost']) 
        else 'ANN' if row['ANN'] >= row['XGBoost'] 
        else 'XGBoost', axis=1
    )
    
    st.dataframe(comparison_df, use_container_width=True)
    
    # Graphiques comparatifs
    st.write("###  Courbes ROC comparatives")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.write("**SVM**")
        fig1 = create_roc_curve_plot(svm_results['roc_data'], svm_results['metrics']['roc_auc'])
        st.pyplot(fig1)
    
    with col2:
        st.write("**ANN**")
        fig2 = create_ann_plots(ann_results, 'roc_curve')
        st.pyplot(fig2)
    
    with col3:
        st.write("**XGBoost**")
        fig3 = create_xgboost_roc_curve_plot(xgboost_results['roc_data'], xgboost_results['metrics']['roc_auc'])
        st.pyplot(fig3)
    
    # Recommandation bas√©e sur les r√©sultats
    st.write("###  Recommandation")
    
    # Calculer le score moyen pour chaque mod√®le
    svm_avg = sum([svm_results['metrics'][m] for m in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']]) / 5
    ann_avg = sum([ann_results['metrics'][m] for m in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']]) / 5
    xgb_avg = sum([xgboost_results['metrics'][m] for m in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']]) / 5
    
    best_model = max([('SVM', svm_avg), ('ANN', ann_avg), ('XGBoost', xgb_avg)], key=lambda x: x[1])
    
    st.success(f" **Mod√®le recommand√© : {best_model[0]}** avec un score moyen de {best_model[1]:.4f}")
    
    if st.button(" Effacer la comparaison"):
        for key in ['svm_results', 'ann_results', 'xgboost_results', 'comparison_mode']:
            if key in st.session_state:
                del st.session_state[key]
        st.rerun()

# Bouton de nettoyage g√©n√©ral
if any(key in st.session_state for key in ['svm_results', 'ann_results', 'xgboost_results']):
    if st.button(" Effacer tous les r√©sultats"):
        for key in ['svm_results', 'ann_results', 'xgboost_results', 'comparison_mode']:
            if key in st.session_state:
                del st.session_state[key]
        st.rerun()